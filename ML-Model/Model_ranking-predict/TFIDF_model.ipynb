{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The purpose of this code is to build a recommendation system for tourism places based on user ratings and additional information about the places.\n",
        "\n",
        "\n",
        "this code aims to build a recommendation system for tourism places using a collaborative filtering approach and includes both numerical and text data for the recommendation model. The neural network is used to learn embeddings for categorical variables, and the model is trained on the provided data.\n",
        "\n",
        " The recommendation is based on user ratings and additional textual information about the places.\n",
        ""
      ],
      "metadata": {
        "id": "pmoZPYx_MmI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code imports necessary libraries such as Pandas for data manipulation, scikit-learn for label encoding, Keras for building a neural network, and Sastrawi for text preprocessing in Indonesian."
      ],
      "metadata": {
        "id": "tm0A0btfNd0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STAoG1DTJu5J",
        "outputId": "bb749478-b757-49d8-d31b-d24c7bba0572"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m204.8/209.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OX3xM-T8EJXU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data from CSV files (tourism_rating.csv, tourism_with_id.csv, and user.csv) is loaded into Pandas DataFrames.\n"
      ],
      "metadata": {
        "id": "2-0s_5D5No0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_tourism_rating = pd.read_csv('tourism_rating.csv')\n",
        "data_tourism_with_id = pd.read_csv('tourism_with_id.csv')\n",
        "data_user = pd.read_csv('user.csv')"
      ],
      "metadata": {
        "id": "l80DGfaQFq9f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code initializes three text processing tools commonly used in natural language processing (NLP) and text mining tasks.\n",
        "\n",
        "\n",
        "\n",
        "*   TfidfVectorizer for creating TF-IDF representations of text data with a limit on the number of features.\n",
        "\n",
        "*   A stemming tool from the Sastrawi library for reducing words to their base forms.\n",
        "\n",
        "*   A stopword remover tool from the Sastrawi library for removing common words that might not contribute much to the analysis.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Oi-yRy1pN3mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tv = TfidfVectorizer(max_features=5000)\n",
        "stem = StemmerFactory().create_stemmer()\n",
        "stopword = StopWordRemoverFactory().create_stop_word_remover()"
      ],
      "metadata": {
        "id": "OG-p8-2qFt4b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Unnecessary columns are dropped from data_tourism_with_id."
      ],
      "metadata": {
        "id": "O310MfeOOUX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_tourism_with_id.drop(['Time_Minutes', 'Coordinate', 'Unnamed: 11', 'Unnamed: 12'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "KUjS2Um4FvnN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average ratings for each place are calculated and merged with the original tourism data based on the 'Place_Id'.\n",
        "making new column on dataframe by merge average_ratings, data_tourism_with_id, on place_id column"
      ],
      "metadata": {
        "id": "jPx-XfBkObHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_ratings = data_tourism_rating.groupby('Place_Id')['Place_Ratings'].mean().reset_index()"
      ],
      "metadata": {
        "id": "a8YfSdCVF0Ro"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_rekomendasi = pd.merge(average_ratings, data_tourism_with_id, on='Place_Id')"
      ],
      "metadata": {
        "id": "bJqK1E0OF2Q1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Combine relevant text data ('Description' and 'Category') into a new 'Tags' column.\n",
        "*   Drop unnecessary columns ('Price', 'Place_Ratings', 'Description').\n",
        "\n",
        "*   Apply text preprocessing techniques, including stemming and stopword removal, to the 'Tags' column.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g-SljFmoPYKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(data):\n",
        "    if isinstance(data, str):  # Check if data is a string\n",
        "        data = data.lower()\n",
        "        data = stem.stem(data)\n",
        "        data = stopword.remove(data)\n",
        "    return data\n",
        "\n",
        "data_tempat = data_rekomendasi.copy()\n",
        "data_tempat['Tags'] = data_tempat['Description'] + ' ' + data_tempat['Category']\n",
        "data_tempat.drop(['Price', 'Place_Ratings', 'Description'], axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "pGfIsxi0PrYq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_tempat.Tags = data_tempat.Tags.apply(preprocessing)"
      ],
      "metadata": {
        "id": "4DVVlfWOKIqq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the preprocessed data into training and testing sets using the train_test_split function."
      ],
      "metadata": {
        "id": "1pz7zVOpQDX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = train_test_split(data_tempat, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Y4UFVXI8KceT"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A neural network model is defined using Keras with three input layers, label encoding, embedding layers, and dense layers. The model is compiled, and then it is trained on the training dataset.\n",
        "\n",
        "*   Define a neural network model using Keras with three input layers for categorical variables ('Place_Name', 'Category', 'City').\n",
        "\n",
        "*   Apply label encoding to categorical variables and use embedding layers to learn dense representations for these variables.\n",
        "\n",
        "*   Concatenate the embeddings and add dense layers to the model.\n"
      ],
      "metadata": {
        "id": "8MRxX67gQKX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 10\n",
        "\n",
        "input_place_name = Input(shape=(1,), name='place_name_input')\n",
        "input_category = Input(shape=(1,), name='category_input')\n",
        "input_city = Input(shape=(1,), name='city_input')\n",
        "\n",
        "\n",
        "place_name_encoder = LabelEncoder()\n",
        "category_encoder = LabelEncoder()\n",
        "city_encoder = LabelEncoder()\n",
        "\n",
        "\n",
        "train_data['Place_Name'] = place_name_encoder.fit_transform(train_data['Place_Name'])\n",
        "train_data['Category'] = category_encoder.fit_transform(train_data['Category'])\n",
        "train_data['City'] = city_encoder.fit_transform(train_data['City'])\n",
        "\n",
        "\n",
        "embedding_place_name = Embedding(train_data['Place_Name'].nunique(), embedding_size)(input_place_name)\n",
        "embedding_category = Embedding(train_data['Category'].nunique(), embedding_size)(input_category)\n",
        "embedding_city = Embedding(train_data['City'].nunique(), embedding_size)(input_city)\n",
        "\n",
        "flatten_place_name = Flatten()(embedding_place_name)\n",
        "flatten_category = Flatten()(embedding_category)\n",
        "flatten_city = Flatten()(embedding_city)\n",
        "\n",
        "concatenated_inputs = Concatenate()([flatten_place_name, flatten_category, flatten_city])"
      ],
      "metadata": {
        "id": "FLx6N0QxKf9o"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense1 = Dense(128, activation='relu')(concatenated_inputs)\n",
        "dense2 = Dense(64, activation='relu')(dense1)\n",
        "output_layer = Dense(1, activation='linear', name='output')(dense2)\n",
        "\n",
        "embedding_model = Model(inputs=[input_place_name, input_category, input_city], outputs=output_layer)\n",
        "embedding_model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "embedding_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6J-uYk6Kxmy",
        "outputId": "e61b4e52-4c32-4f14-fb7b-c71ff62ff577"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " place_name_input (InputLay  [(None, 1)]                  0         []                            \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " category_input (InputLayer  [(None, 1)]                  0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " city_input (InputLayer)     [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 1, 10)                3490      ['place_name_input[0][0]']    \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 1, 10)                60        ['category_input[0][0]']      \n",
            "                                                                                                  \n",
            " embedding_2 (Embedding)     (None, 1, 10)                50        ['city_input[0][0]']          \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 10)                   0         ['embedding[0][0]']           \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 10)                   0         ['embedding_1[0][0]']         \n",
            "                                                                                                  \n",
            " flatten_2 (Flatten)         (None, 10)                   0         ['embedding_2[0][0]']         \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 30)                   0         ['flatten[0][0]',             \n",
            "                                                                     'flatten_1[0][0]',           \n",
            "                                                                     'flatten_2[0][0]']           \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 128)                  3968      ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 64)                   8256      ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " output (Dense)              (None, 1)                    65        ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 15889 (62.07 KB)\n",
            "Trainable params: 15889 (62.07 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile the model with the mean squared error loss function and the Adam optimizer.\n",
        "Train the model on the training dataset for 10 epochs."
      ],
      "metadata": {
        "id": "45drrIKaQk35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "X_train = [train_data['Place_Name'].values, train_data['Category'].values, train_data['City'].values]\n",
        "y_train = train_data['Rating'].values\n",
        "\n",
        "embedding_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PriLNin-K_yV",
        "outputId": "c045e5f0-d59f-40d5-a5e9-20c7673e2ef2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "9/9 [==============================] - 1s 25ms/step - loss: 19.2511 - val_loss: 18.3515\n",
            "Epoch 2/10\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 17.4660 - val_loss: 16.0362\n",
            "Epoch 3/10\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 14.3891 - val_loss: 12.0704\n",
            "Epoch 4/10\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 9.3962 - val_loss: 6.1988\n",
            "Epoch 5/10\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 3.2799 - val_loss: 0.8992\n",
            "Epoch 6/10\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4426 - val_loss: 0.5538\n",
            "Epoch 7/10\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.6314 - val_loss: 0.1641\n",
            "Epoch 8/10\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1175 - val_loss: 0.2835\n",
            "Epoch 9/10\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1360 - val_loss: 0.1828\n",
            "Epoch 10/10\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0518 - val_loss: 0.0873\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7931f9ace0b0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this line is for saving few file tha for another source in this project\n",
        "\n",
        "*   Save the trained neural network model to an HDF5 file (embedding_model.h5).\n",
        "*   Save the test data to a pickle file (test_data.pkl).\n",
        "*   Save the class labels learned during label encoding for 'Place_Name', 'Category', and 'City' to CSV files (place_name_encoder_classes.csv, category_encoder_classes.csv, city_encoder_classes.csv).\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Rw2-Xs1OS2NP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model to a file\n",
        "embedding_model.save('embedding_model.h5')\n",
        "\n",
        "# Save the test data to a pickle file\n",
        "test_data.to_pickle('test_data.pkl')\n",
        "place_name_encoder_df = pd.DataFrame({'class': place_name_encoder.classes_})\n",
        "place_name_encoder_df.to_csv('place_name_encoder_classes.csv', index=False)\n",
        "\n",
        "category_encoder_df = pd.DataFrame({'class': category_encoder.classes_})\n",
        "category_encoder_df.to_csv('category_encoder_classes.csv', index=False)\n",
        "\n",
        "city_encoder_df = pd.DataFrame({'class': city_encoder.classes_})\n",
        "city_encoder_df.to_csv('city_encoder_classes.csv', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-fXfz2NLByd",
        "outputId": "b670f4cc-0e49-4234-d3c1-c39fef5ade24"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    }
  ]
}